{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2b229ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight\n",
      "\n",
      "model.layers.0.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.0.self_attn.q_proj.bias\n",
      "\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.0.self_attn.k_proj.bias\n",
      "\n",
      "model.layers.0.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.0.self_attn.v_proj.bias\n",
      "\n",
      "model.layers.0.self_attn.dense.weight\n",
      "\n",
      "model.layers.0.self_attn.dense.bias\n",
      "\n",
      "model.layers.0.mlp.fc1.weight\n",
      "\n",
      "model.layers.0.mlp.fc1.bias\n",
      "\n",
      "model.layers.0.mlp.fc2.weight\n",
      "\n",
      "model.layers.0.mlp.fc2.bias\n",
      "\n",
      "model.layers.0.input_layernorm.weight\n",
      "\n",
      "model.layers.0.input_layernorm.bias\n",
      "\n",
      "model.layers.1.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.1.self_attn.q_proj.bias\n",
      "\n",
      "model.layers.1.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.1.self_attn.k_proj.bias\n",
      "\n",
      "model.layers.1.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.1.self_attn.v_proj.bias\n",
      "\n",
      "model.layers.1.self_attn.dense.weight\n",
      "\n",
      "model.layers.1.self_attn.dense.bias\n",
      "\n",
      "model.layers.1.mlp.fc1.weight\n",
      "\n",
      "model.layers.1.mlp.fc1.bias\n",
      "\n",
      "model.layers.1.mlp.fc2.weight\n",
      "\n",
      "model.layers.1.mlp.fc2.bias\n",
      "\n",
      "model.layers.1.input_layernorm.weight\n",
      "\n",
      "model.layers.1.input_layernorm.bias\n",
      "\n",
      "model.layers.2.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.2.self_attn.q_proj.bias\n",
      "\n",
      "model.layers.2.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.2.self_attn.k_proj.bias\n",
      "\n",
      "model.layers.2.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.2.self_attn.v_proj.bias\n",
      "\n",
      "model.layers.2.self_attn.dense.weight\n",
      "\n",
      "model.layers.2.self_attn.dense.bias\n",
      "\n",
      "model.layers.2.mlp.fc1.weight\n",
      "\n",
      "model.layers.2.mlp.fc1.bias\n",
      "\n",
      "model.layers.2.mlp.fc2.weight\n",
      "\n",
      "model.layers.2.mlp.fc2.bias\n",
      "\n",
      "model.layers.2.input_layernorm.weight\n",
      "\n",
      "model.layers.2.input_layernorm.bias\n",
      "\n",
      "model.layers.3.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.3.self_attn.q_proj.bias\n",
      "\n",
      "model.layers.3.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.3.self_attn.k_proj.bias\n",
      "\n",
      "model.layers.3.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.3.self_attn.v_proj.bias\n",
      "\n",
      "model.layers.3.self_attn.dense.weight\n",
      "\n",
      "model.layers.3.self_attn.dense.bias\n",
      "\n",
      "model.layers.3.mlp.fc1.weight\n",
      "\n",
      "model.layers.3.mlp.fc1.bias\n",
      "\n",
      "model.layers.3.mlp.fc2.weight\n",
      "\n",
      "model.layers.3.mlp.fc2.bias\n",
      "\n",
      "model.layers.3.input_layernorm.weight\n",
      "\n",
      "model.layers.3.input_layernorm.bias\n",
      "\n",
      "model.layers.4.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.4.self_attn.q_proj.bias\n",
      "\n",
      "model.layers.4.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.4.self_attn.k_proj.bias\n",
      "\n",
      "model.layers.4.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.4.self_attn.v_proj.bias\n",
      "\n",
      "model.layers.4.self_attn.dense.weight\n",
      "\n",
      "model.layers.4.self_attn.dense.bias\n",
      "\n",
      "model.layers.4.mlp.fc1.weight\n",
      "\n",
      "model.layers.4.mlp.fc1.bias\n",
      "\n",
      "model.layers.4.mlp.fc2.weight\n",
      "\n",
      "model.layers.4.mlp.fc2.bias\n",
      "\n",
      "model.layers.4.input_layernorm.weight\n",
      "\n",
      "model.layers.4.input_layernorm.bias\n",
      "\n",
      "model.layers.5.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.5.self_attn.q_proj.bias\n",
      "\n",
      "model.layers.5.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.5.self_attn.k_proj.bias\n",
      "\n",
      "model.layers.5.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.5.self_attn.v_proj.bias\n",
      "\n",
      "model.layers.5.self_attn.dense.weight\n",
      "\n",
      "model.layers.5.self_attn.dense.bias\n",
      "\n",
      "model.layers.5.mlp.fc1.weight\n",
      "\n",
      "model.layers.5.mlp.fc1.bias\n",
      "\n",
      "model.layers.5.mlp.fc2.weight\n",
      "\n",
      "model.layers.5.mlp.fc2.bias\n",
      "\n",
      "model.layers.5.input_layernorm.weight\n",
      "\n",
      "model.layers.5.input_layernorm.bias\n",
      "\n",
      "model.layers.6.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.6.self_attn.q_proj.bias\n",
      "\n",
      "model.layers.6.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.6.self_attn.k_proj.bias\n",
      "\n",
      "model.layers.6.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.6.self_attn.v_proj.bias\n",
      "\n",
      "model.layers.6.self_attn.dense.weight\n",
      "\n",
      "model.layers.6.self_attn.dense.bias\n",
      "\n",
      "model.layers.6.mlp.fc1.weight\n",
      "\n",
      "model.layers.6.mlp.fc1.bias\n",
      "\n",
      "model.layers.6.mlp.fc2.weight\n",
      "\n",
      "model.layers.6.mlp.fc2.bias\n",
      "\n",
      "model.layers.6.input_layernorm.weight\n",
      "\n",
      "model.layers.6.input_layernorm.bias\n",
      "\n",
      "model.layers.7.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.7.self_attn.q_proj.bias\n",
      "\n",
      "model.layers.7.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.7.self_attn.k_proj.bias\n",
      "\n",
      "model.layers.7.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.7.self_attn.v_proj.bias\n",
      "\n",
      "model.layers.7.self_attn.dense.weight\n",
      "\n",
      "model.layers.7.self_attn.dense.bias\n",
      "\n",
      "model.layers.7.mlp.fc1.weight\n",
      "\n",
      "model.layers.7.mlp.fc1.bias\n",
      "\n",
      "model.layers.7.mlp.fc2.weight\n",
      "\n",
      "model.layers.7.mlp.fc2.bias\n",
      "\n",
      "model.layers.7.input_layernorm.weight\n",
      "\n",
      "model.layers.7.input_layernorm.bias\n",
      "\n",
      "model.layers.8.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.8.self_attn.q_proj.bias\n",
      "\n",
      "model.layers.8.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.8.self_attn.k_proj.bias\n",
      "\n",
      "model.layers.8.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.8.self_attn.v_proj.bias\n",
      "\n",
      "model.layers.8.self_attn.dense.weight\n",
      "\n",
      "model.layers.8.self_attn.dense.bias\n",
      "\n",
      "model.layers.8.mlp.fc1.weight\n",
      "\n",
      "model.layers.8.mlp.fc1.bias\n",
      "\n",
      "model.layers.8.mlp.fc2.weight\n",
      "\n",
      "model.layers.8.mlp.fc2.bias\n",
      "\n",
      "model.layers.8.input_layernorm.weight\n",
      "\n",
      "model.layers.8.input_layernorm.bias\n",
      "\n",
      "model.layers.9.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.9.self_attn.q_proj.bias\n",
      "\n",
      "model.layers.9.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.9.self_attn.k_proj.bias\n",
      "\n",
      "model.layers.9.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.9.self_attn.v_proj.bias\n",
      "\n",
      "model.layers.9.self_attn.dense.weight\n",
      "\n",
      "model.layers.9.self_attn.dense.bias\n",
      "\n",
      "model.layers.9.mlp.fc1.weight\n",
      "\n",
      "model.layers.9.mlp.fc1.bias\n",
      "\n",
      "model.layers.9.mlp.fc2.weight\n",
      "\n",
      "model.layers.9.mlp.fc2.bias\n",
      "\n",
      "model.layers.9.input_layernorm.weight\n",
      "\n",
      "model.layers.9.input_layernorm.bias\n",
      "\n",
      "model.layers.10.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.10.self_attn.q_proj.bias\n",
      "\n",
      "model.layers.10.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.10.self_attn.k_proj.bias\n",
      "\n",
      "model.layers.10.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.10.self_attn.v_proj.bias\n",
      "\n",
      "model.layers.10.self_attn.dense.weight\n",
      "\n",
      "model.layers.10.self_attn.dense.bias\n",
      "\n",
      "model.layers.10.mlp.fc1.weight\n",
      "\n",
      "model.layers.10.mlp.fc1.bias\n",
      "\n",
      "model.layers.10.mlp.fc2.weight\n",
      "\n",
      "model.layers.10.mlp.fc2.bias\n",
      "\n",
      "model.layers.10.input_layernorm.weight\n",
      "\n",
      "model.layers.10.input_layernorm.bias\n",
      "\n",
      "model.layers.11.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.11.self_attn.q_proj.bias\n",
      "\n",
      "model.layers.11.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.11.self_attn.k_proj.bias\n",
      "\n",
      "model.layers.11.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.11.self_attn.v_proj.bias\n",
      "\n",
      "model.layers.11.self_attn.dense.weight\n",
      "\n",
      "model.layers.11.self_attn.dense.bias\n",
      "\n",
      "model.layers.11.mlp.fc1.weight\n",
      "\n",
      "model.layers.11.mlp.fc1.bias\n",
      "\n",
      "model.layers.11.mlp.fc2.weight\n",
      "\n",
      "model.layers.11.mlp.fc2.bias\n",
      "\n",
      "model.layers.11.input_layernorm.weight\n",
      "\n",
      "model.layers.11.input_layernorm.bias\n",
      "\n",
      "model.layers.12.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.12.self_attn.q_proj.bias\n",
      "\n",
      "model.layers.12.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.12.self_attn.k_proj.bias\n",
      "\n",
      "model.layers.12.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.12.self_attn.v_proj.bias\n",
      "\n",
      "model.layers.12.self_attn.dense.weight\n",
      "\n",
      "model.layers.12.self_attn.dense.bias\n",
      "\n",
      "model.layers.12.mlp.fc1.weight\n",
      "\n",
      "model.layers.12.mlp.fc1.bias\n",
      "\n",
      "model.layers.12.mlp.fc2.weight\n",
      "\n",
      "model.layers.12.mlp.fc2.bias\n",
      "\n",
      "model.layers.12.input_layernorm.weight\n",
      "\n",
      "model.layers.12.input_layernorm.bias\n",
      "\n",
      "model.layers.13.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.13.self_attn.q_proj.bias\n",
      "\n",
      "model.layers.13.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.13.self_attn.k_proj.bias\n",
      "\n",
      "model.layers.13.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.13.self_attn.v_proj.bias\n",
      "\n",
      "model.layers.13.self_attn.dense.weight\n",
      "\n",
      "model.layers.13.self_attn.dense.bias\n",
      "\n",
      "model.layers.13.mlp.fc1.weight\n",
      "\n",
      "model.layers.13.mlp.fc1.bias\n",
      "\n",
      "model.layers.13.mlp.fc2.weight\n",
      "\n",
      "model.layers.13.mlp.fc2.bias\n",
      "\n",
      "model.layers.13.input_layernorm.weight\n",
      "\n",
      "model.layers.13.input_layernorm.bias\n",
      "\n",
      "model.layers.14.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.14.self_attn.q_proj.bias\n",
      "\n",
      "model.layers.14.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.14.self_attn.k_proj.bias\n",
      "\n",
      "model.layers.14.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.14.self_attn.v_proj.bias\n",
      "\n",
      "model.layers.14.self_attn.dense.weight\n",
      "\n",
      "model.layers.14.self_attn.dense.bias\n",
      "\n",
      "model.layers.14.mlp.fc1.weight\n",
      "\n",
      "model.layers.14.mlp.fc1.bias\n",
      "\n",
      "model.layers.14.mlp.fc2.weight\n",
      "\n",
      "model.layers.14.mlp.fc2.bias\n",
      "\n",
      "model.layers.14.input_layernorm.weight\n",
      "\n",
      "model.layers.14.input_layernorm.bias\n",
      "\n",
      "model.layers.15.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.15.self_attn.q_proj.bias\n",
      "\n",
      "model.layers.15.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.15.self_attn.k_proj.bias\n",
      "\n",
      "model.layers.15.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.15.self_attn.v_proj.bias\n",
      "\n",
      "model.layers.15.self_attn.dense.weight\n",
      "\n",
      "model.layers.15.self_attn.dense.bias\n",
      "\n",
      "model.layers.15.mlp.fc1.weight\n",
      "\n",
      "model.layers.15.mlp.fc1.bias\n",
      "\n",
      "model.layers.15.mlp.fc2.weight\n",
      "\n",
      "model.layers.15.mlp.fc2.bias\n",
      "\n",
      "model.layers.15.input_layernorm.weight\n",
      "\n",
      "model.layers.15.input_layernorm.bias\n",
      "\n",
      "model.layers.16.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.16.self_attn.q_proj.bias\n",
      "\n",
      "model.layers.16.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.16.self_attn.k_proj.bias\n",
      "\n",
      "model.layers.16.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.16.self_attn.v_proj.bias\n",
      "\n",
      "model.layers.16.self_attn.dense.weight\n",
      "\n",
      "model.layers.16.self_attn.dense.bias\n",
      "\n",
      "model.layers.16.mlp.fc1.weight\n",
      "\n",
      "model.layers.16.mlp.fc1.bias\n",
      "\n",
      "model.layers.16.mlp.fc2.weight\n",
      "\n",
      "model.layers.16.mlp.fc2.bias\n",
      "\n",
      "model.layers.16.input_layernorm.weight\n",
      "\n",
      "model.layers.16.input_layernorm.bias\n",
      "\n",
      "model.layers.17.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.17.self_attn.q_proj.bias\n",
      "\n",
      "model.layers.17.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.17.self_attn.k_proj.bias\n",
      "\n",
      "model.layers.17.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.17.self_attn.v_proj.bias\n",
      "\n",
      "model.layers.17.self_attn.dense.weight\n",
      "\n",
      "model.layers.17.self_attn.dense.bias\n",
      "\n",
      "model.layers.17.mlp.fc1.weight\n",
      "\n",
      "model.layers.17.mlp.fc1.bias\n",
      "\n",
      "model.layers.17.mlp.fc2.weight\n",
      "\n",
      "model.layers.17.mlp.fc2.bias\n",
      "\n",
      "model.layers.17.input_layernorm.weight\n",
      "\n",
      "model.layers.17.input_layernorm.bias\n",
      "\n",
      "model.layers.18.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.18.self_attn.q_proj.bias\n",
      "\n",
      "model.layers.18.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.18.self_attn.k_proj.bias\n",
      "\n",
      "model.layers.18.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.18.self_attn.v_proj.bias\n",
      "\n",
      "model.layers.18.self_attn.dense.weight\n",
      "\n",
      "model.layers.18.self_attn.dense.bias\n",
      "\n",
      "model.layers.18.mlp.fc1.weight\n",
      "\n",
      "model.layers.18.mlp.fc1.bias\n",
      "\n",
      "model.layers.18.mlp.fc2.weight\n",
      "\n",
      "model.layers.18.mlp.fc2.bias\n",
      "\n",
      "model.layers.18.input_layernorm.weight\n",
      "\n",
      "model.layers.18.input_layernorm.bias\n",
      "\n",
      "model.layers.19.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.19.self_attn.q_proj.bias\n",
      "\n",
      "model.layers.19.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.19.self_attn.k_proj.bias\n",
      "\n",
      "model.layers.19.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.19.self_attn.v_proj.bias\n",
      "\n",
      "model.layers.19.self_attn.dense.weight\n",
      "\n",
      "model.layers.19.self_attn.dense.bias\n",
      "\n",
      "model.layers.19.mlp.fc1.weight\n",
      "\n",
      "model.layers.19.mlp.fc1.bias\n",
      "\n",
      "model.layers.19.mlp.fc2.weight\n",
      "\n",
      "model.layers.19.mlp.fc2.bias\n",
      "\n",
      "model.layers.19.input_layernorm.weight\n",
      "\n",
      "model.layers.19.input_layernorm.bias\n",
      "\n",
      "model.layers.20.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.20.self_attn.q_proj.bias\n",
      "\n",
      "model.layers.20.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.20.self_attn.k_proj.bias\n",
      "\n",
      "model.layers.20.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.20.self_attn.v_proj.bias\n",
      "\n",
      "model.layers.20.self_attn.dense.weight\n",
      "\n",
      "model.layers.20.self_attn.dense.bias\n",
      "\n",
      "model.layers.20.mlp.fc1.weight\n",
      "\n",
      "model.layers.20.mlp.fc1.bias\n",
      "\n",
      "model.layers.20.mlp.fc2.weight\n",
      "\n",
      "model.layers.20.mlp.fc2.bias\n",
      "\n",
      "model.layers.20.input_layernorm.weight\n",
      "\n",
      "model.layers.20.input_layernorm.bias\n",
      "\n",
      "model.layers.21.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.21.self_attn.q_proj.bias\n",
      "\n",
      "model.layers.21.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.21.self_attn.k_proj.bias\n",
      "\n",
      "model.layers.21.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.21.self_attn.v_proj.bias\n",
      "\n",
      "model.layers.21.self_attn.dense.weight\n",
      "\n",
      "model.layers.21.self_attn.dense.bias\n",
      "\n",
      "model.layers.21.mlp.fc1.weight\n",
      "\n",
      "model.layers.21.mlp.fc1.bias\n",
      "\n",
      "model.layers.21.mlp.fc2.weight\n",
      "\n",
      "model.layers.21.mlp.fc2.bias\n",
      "\n",
      "model.layers.21.input_layernorm.weight\n",
      "\n",
      "model.layers.21.input_layernorm.bias\n",
      "\n",
      "model.layers.22.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.22.self_attn.q_proj.bias\n",
      "\n",
      "model.layers.22.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.22.self_attn.k_proj.bias\n",
      "\n",
      "model.layers.22.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.22.self_attn.v_proj.bias\n",
      "\n",
      "model.layers.22.self_attn.dense.weight\n",
      "\n",
      "model.layers.22.self_attn.dense.bias\n",
      "\n",
      "model.layers.22.mlp.fc1.weight\n",
      "\n",
      "model.layers.22.mlp.fc1.bias\n",
      "\n",
      "model.layers.22.mlp.fc2.weight\n",
      "\n",
      "model.layers.22.mlp.fc2.bias\n",
      "\n",
      "model.layers.22.input_layernorm.weight\n",
      "\n",
      "model.layers.22.input_layernorm.bias\n",
      "\n",
      "model.layers.23.self_attn.q_proj.weight\n",
      "\n",
      "model.layers.23.self_attn.q_proj.bias\n",
      "\n",
      "model.layers.23.self_attn.k_proj.weight\n",
      "\n",
      "model.layers.23.self_attn.k_proj.bias\n",
      "\n",
      "model.layers.23.self_attn.v_proj.weight\n",
      "\n",
      "model.layers.23.self_attn.v_proj.bias\n",
      "\n",
      "model.layers.23.self_attn.dense.weight\n",
      "\n",
      "model.layers.23.self_attn.dense.bias\n",
      "\n",
      "model.layers.23.mlp.fc1.weight\n",
      "\n",
      "model.layers.23.mlp.fc1.bias\n",
      "\n",
      "model.layers.23.mlp.fc2.weight\n",
      "\n",
      "model.layers.23.mlp.fc2.bias\n",
      "\n",
      "model.layers.23.input_layernorm.weight\n",
      "\n",
      "model.layers.23.input_layernorm.bias\n",
      "\n",
      "model.final_layernorm.weight\n",
      "\n",
      "model.final_layernorm.bias\n",
      "\n",
      "lm_head.weight\n",
      "\n",
      "lm_head.bias\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", torch_dtype=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n",
    "\n",
    "for key in model.state_dict().keys():\n",
    "    print(key)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fb55bf",
   "metadata": {},
   "source": [
    "# Loading the Config for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89962e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type phi to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PretrainedConfig {\n",
       "  \"architectures\": [\n",
       "    \"PhiForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"embd_pdrop\": 0.0,\n",
       "  \"hidden_act\": \"gelu_new\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 8192,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"num_key_value_heads\": null,\n",
       "  \"partial_rotary_factor\": 0.5,\n",
       "  \"qk_layernorm\": false,\n",
       "  \"resid_pdrop\": 0.0,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.53.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 51200\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "config = PretrainedConfig.from_pretrained(\"microsoft/phi-1_5\")\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef6e42b",
   "metadata": {},
   "source": [
    "# Input & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bd82102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: tensor([[15496,    11,   466,   345,   760,   546, 21123,  1236,  1976, 17167,\n",
      "          2163,    30]])\n",
      "Attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n",
    "text = \"Hello, do you know about reimann zeta function?\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokenized_text = tokenizer(text, return_tensors=\"pt\")\n",
    "tokens, attention_mask = tokenized_text[\"input_ids\"], tokenized_text[\"attention_mask\"]\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Attention mask:\", attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47429be",
   "metadata": {},
   "source": [
    "# Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a7f81f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    \"\"\"Token embedding with dropout.\"\"\"\n",
    "\n",
    "    def __init__(self, config: PretrainedConfig) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.hidden_size) # Token embedding\n",
    "        # Loading the token embedding weights from the model    \n",
    "        self.wte.weight.data = model.model.embed_tokens.weight.data.clone()\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "\n",
    "    def forward(self, input_ids: torch.LongTensor) -> torch.FloatTensor:\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "\n",
    "        hidden_states = self.wte(input_ids)\n",
    "        hidden_states = self.drop(hidden_states)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe685f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.7046e-03, -1.5488e-02,  6.0272e-02,  ...,  9.5520e-03,\n",
       "         -5.4169e-02, -5.8174e-03],\n",
       "        [ 2.4323e-02,  5.4321e-02,  1.7776e-02,  ...,  2.5421e-02,\n",
       "         -4.3854e-02,  3.9612e-02],\n",
       "        [-4.1565e-02,  3.6987e-02, -1.5976e-02,  ...,  4.7394e-02,\n",
       "         -1.6113e-02,  4.3716e-03],\n",
       "        ...,\n",
       "        [-1.5259e-05,  3.0160e-05, -1.6034e-05,  ..., -1.9729e-05,\n",
       "         -1.3590e-05,  9.2745e-05],\n",
       "        [-8.0466e-06, -2.6107e-05, -5.1260e-05,  ...,  4.0054e-05,\n",
       "          4.9233e-05, -1.6689e-05],\n",
       "        [ 3.2783e-06, -1.7822e-05,  2.4676e-05,  ..., -3.4511e-05,\n",
       "         -2.0921e-05,  1.6928e-05]], dtype=torch.float16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.embed_tokens.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5a57881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 2048])\n"
     ]
    }
   ],
   "source": [
    "embedding = Embedding(config)\n",
    "token_embed = embedding(tokens)\n",
    "print(token_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a463990",
   "metadata": {},
   "source": [
    "# Scaled Dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04caf42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d167b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 2048])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a14717b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
